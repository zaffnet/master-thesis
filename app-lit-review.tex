\chapter{Summary of Literature Survey on Synthetic Doppelgängers}
\label{app:lit-review}

{ % Start a group to keep the arrayrulecolor local
\arrayrulecolor{lightgray}
\begin{longtable}{| L{4.5cm} | L{3.5cm} | L{3.5cm} | L{3.5cm} |}

%================== CAPTION AND HEADERS ==================
\caption{Validation Methods for LLM-based Synthetic Subjects}
\label{tab:llm_validation}\\

% --- Header for the first page ---
\hline
\rowcolor{tablehead}
\textbf{Publication} & \textbf{Installation Method} & \textbf{Validation Approach} & \textbf{Limitation of Validation}\\
\hline
\endfirsthead

% --- Header for subsequent pages ---
\hline
\rowcolor{tablehead}
\textbf{Publication} & \textbf{Installation Method} & \textbf{Validation Approach} & \textbf{Limitation of Validation}\\
\hline
\endhead

% --- Footer for all pages except the last ---
\endfoot

% --- Footer for the last page ---
\endlastfoot

%================== TABLE BODY (SORTED BY YEAR) ==================
\citet{andreas-2022-language}. \href{https://aclanthology.org/2022.findings-emnlp.423/}{\textit{Language Models as Agent Models}}. & Not applicable. The paper is a conceptual review and position paper. & The paper synthesizes and reinterprets existing research, using findings from prior literature as evidence for its conceptual claims about agent modeling. & The argument is based on a literature survey and conceptual reasoning, not on novel empirical validation or a quantitative simulation.
\\\hline
\citet{aher2023}. \href{https://proceedings.mlr.press/v202/aher23a.html}{\textit{Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies}}. & Zero-shot prompting of LLMs to simulate participants in replications of classic behavioural experiments (e.g., Ultimatum Game, Milgram experiment). & The paper introduces the `Turing Experiment' methodology, where the aggregate behavioural distributions of LLM simulations are quantitatively compared to the results of the original human studies. & While the models replicate high-level outcomes, the validation does not guarantee process fidelity. A `hyper-accuracy distortion' is noted in some models, revealing a non-humanlike bias.
\\\hline
\citet{argyle2023}. \href{https://doi.org/10.1017/pan.2023.2}{\textit{Out of one, many: Using language models to simulate human samples}}. & Conditioning GPT-3 with thousands of socio-demographic backstories from real human survey participants. & Comparison of the response distributions of the ``silicon sample'' to the human sample to assess ``algorithmic fidelity'' at the population level. & The validation focuses on aggregate statistical patterns, not on the fidelity of individual-level responses or the realism of conversational interaction.
\\\hline
\citet{ayers2023social}. \href{https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309}{\textit{Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions}}. & A chatbot (ChatGPT) and physicians were prompted with real-world patient questions posted on a public social media forum. & A panel of licensed healthcare professionals blindly evaluated the quality and empathy of both human and AI responses. & The Turing-style test focuses on the quality of static, written advice, not on interactive diagnostic capabilities or the ability to manage a longitudinal patient relationship.
\\\hline
\citet{binz2023using}. \href{https://doi.org/10.1073/pnas.2218523120}{\textit{Using cognitive psychology to understand GPT-3}}. & Zero-shot prompting of GPT-3 with classic cognitive bias scenarios (e.g., the Linda problem). & Comparison of the model's pattern of choices and errors against the well-documented results from human experiments. & The validation shows the model reproduces human-like biases in constrained tasks but does not prove the underlying cognitive mechanisms are the same, or that this behaviour generalizes to novel tasks.
\\\hline
\citet{he2024agentscourt}. \href{https://arxiv.org/abs/2403.02959}{\textit{AgentsCourt: Building Judicial Decision-Making Agents with Court Debate Simulation and Legal Knowledge Augmentation}}. & Agents are assigned roles (e.g., judge, plaintiff, defendant) and given case evidence to simulate court debates and deliberate a verdict. & Comparison of the LLM jury's final verdict and legal reasoning to the outcomes from human mock jury studies or real-world legal precedents. & The validation focuses on the legal and logical outputs, but fails to model the emotional, social, and group dynamics that heavily influence real jury deliberations.
\\\hline
\citet{gao2023s3}. \href{https://arxiv.org/abs/2307.14984}{\textit{S³: Social-network Simulation System with Large Language Model-Empowered Agents}}. & Agents are instantiated in a social network simulation system (S³) using prompt engineering and tuning to emulate human emotion, attitude, and interaction behaviours. & Evaluation of emergent population-level phenomena (e.g., propagation of information, attitudes, and emotions) against real-world social network data. & The paper describes initial work and focuses on the system's ability to reproduce emergent phenomena, with less emphasis on the micro-level fidelity of individual agent reasoning or behaviour.
\\\hline
\citet{horton2023large}. \href{https://www.nber.org/papers/w31122}{\textit{Large Language Models as Simulated Economic Agents: What Can We Learn from Them?}}. & LLMs are instructed to act as players in classic economic games, being given endowments, information, and preferences. & Comparison of LLM decisions to established patterns of human behaviour, showing qualitatively similar results to original laboratory experiments. & The validation confirms replication of known qualitative outcomes but does not validate the underlying cognitive processes, suggesting its primary use is for piloting studies, not replacing them.
\\\hline
\citet{kosinski2023evaluating}. \href{https://arxiv.org/abs/2302.02083}{\textit{Evaluating Large Language Models in Theory of Mind Tasks}}. & Zero-shot prompting on classic false-belief tasks. & Performance on standardized psychological tests for Theory of Mind (ToM), comparing newer models (e.g., GPT-4) to older versions and child development benchmarks. & Tests puzzle-solving ability on static, text-based scenarios, which may not equate to genuine social inference or the imputation of mental states in dynamic, real-world interactions.
\\\hline
\citet{alam2023integrating}. \href{https://doi.org/10.3389/fmed.2023.1279707}{\textit{Integrating AI in medical education: embracing ethical usage and critical understanding}}. & Not described (Review paper). & Discusses prevalent validation methods in the field, such as performance on medical exams, user satisfaction, and expert review. & The review highlights a field-wide limitation: a lack of rigorous, comparative validation studies that benchmark AI performance systematically against human experts.
\\\hline
\citet{Park2023}. \href{https://arxiv.org/abs/2304.03442}{\textit{Generative Agents: Interactive Simulacra of Human Behaviour}}. & Agents are instantiated with seed memories and personalities via natural language prompts in an interactive sandbox environment. & Ablation studies on agent architecture components and qualitative author evaluation of ``believability'' of emergent social behaviours. & The validation relies on subjective author assessment of believability and lacks a rigorous, systematic comparison to a quantitative human baseline.
\\\hline
\citet{pmlr-v202-santurkar23a}. \href{https://proceedings.mlr.press/v202/santurkar23a.html}{\textit{Whose Opinions Do Language Models Reflect?}}. & Prompting LLMs with questions from public opinion polls (Pew Research) with and without demographic steering. & Quantitative comparison of the distribution of LM opinions against the responses of 60 U.S. demographic groups using the OpinionQA dataset. & The validation reveals substantial misalignment between LM opinions and U.S. demographic groups, which persists even when the model is explicitly steered toward a target group.
\\\hline
\citet{spatharioti2023comparing}. \href{https://arxiv.org/abs/2307.03744}{\textit{Comparing Traditional and LLM-based Search for Consumer Choice}}. & Participants in a randomized experiment are assigned to use either an LLM-based search tool or a traditional search engine. & Comparison of task completion time, query complexity, user satisfaction, and decision accuracy between the two groups. & Validation shows users with LLMs are faster, but it also reveals a limitation: users exhibit overreliance on the LLM and fail to spot incorrect information when the model errs.
\\\hline
\citet{suh2025language}. \href{https://arxiv.org/abs/2502.16761}{\textit{Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions}}. & Fine-tuning LLMs on SubPOP, a large-scale dataset of public opinion survey questions and responses. & Measuring the reduction in the ``LLM-human gap'' compared to baselines and testing generalization to unseen surveys and subpopulations. & The validation focuses on predicting response distributions from survey data, not on the underlying reasoning or conversational fidelity of the agents.
\\\hline
\citet{laverde2025integrating}. \href{https://doi.org/10.1016/j.csbj.2025.05.025}{\textit{Integrating large language model-based agents into a virtual patient chatbot for clinical anamnesis training}}. & Virtual patients are configured with preset preferences, histories, and personalities via system prompts for medical training simulations. & Evaluation of the consistency and plausibility of agent responses against the defined case, along with a quantitative user satisfaction score from the Chatbot Usability Questionnaire (CUQ). & The validation method is susceptible to evaluator bias and primarily assesses script-following and information retrieval rather than authentic, spontaneous patient behaviour.
\\\hline
\citet{jiang-etal-2023-personallm}. \href{https://aclanthology.org/2024.findings-naacl.229/}{\textit{PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits}}. & Prompting LLM personas with Big Five personality trait profiles to create ``LLM personas.'' & LLM personas complete the Big Five Inventory (BFI) personality test; human evaluators assess personality from stories written by the personas. & Human accuracy in perceiving the correct personality drops significantly when evaluators are informed of AI authorship, suggesting the perception of personality is fragile.
\\\hline

\end{longtable}
} % End of the \arrayrulecolor scope
