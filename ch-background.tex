\chapter{Background and Related Work}
\label{ch:background}
This chapter establishes the conceptual and empirical foundations for our contributions to automated talk therapy and the development of synthetic agents. We begin by surveying research on the clinical efficacy of motivational interviewing for smoking cessation. We then examine the emergence of transformer-based LLMs as conversational agents, with a focus on recent efforts to develop motivational interviewing counsellor chatbots using LLMs. We then survey methods for constructing synthetic agents, with a particular emphasis on LLM-based synthetic patients designed for behavioural research. We particularly focus on the techniques of persona \emph{installation} via prompting, which seek to \emph{embed} demographic and behavioural characteristics into synthetic agents. We also outline the inherent limitations of prompt-based persona installation, including issues of consistency, depth, and stereotype propagation. Finally, we critically review existing approaches for validating the fidelity of persona installation and compare them with our own validation methods.


\section{Motivational Interviewing (MI)}
Motivational Interviewing (MI) is a counselling technique originally developed in the 1980s to treat alcohol dependence and has since been applied across a wide range of health interventions, including smoking cessation~\cite{Miller1983, MillerRollnick2023}. MI is defined as a collaborative, client-centred conversational method designed to elicit intrinsic motivation for change by helping individuals resolve ambivalence. Instead of confronting or directing the client, the MI practitioner adopts a guiding stance: asking open-ended questions, listening reflectively, and echoing the client's own change-relevant statements. This approach aims to elicit \emph{change talk} (client statements in favour of change) while reducing \emph{sustain talk} (arguments for maintaining the status quo)~\cite{MillerRose2009}. By evoking a person's own reasons for change in a non-judgmental and supportive way, MI strengthens their perceived autonomy and self-efficacy. \Cref{tab:change_sustain_talk} presents examples of change and sustain talk alongside MI-consistent responses by the counsellor.


\begin{table}[ht!]
	\centering
	\begin{tabular}{@{} p{0.12\textwidth} p{0.38\textwidth} p{0.38\textwidth} @{}}
		\toprule
		\textbf{Speaker}                                                                                             & \textbf{Change Talk} & \textbf{Sustain Talk} \\
		\midrule
		\textbf{Client}                                                                                              &
		``I know I should quit smoking because my kids hate the smell, and I don't want them to pick up the habit.'' &
		``I've tried quitting before, but I always end up lighting one when I'm stressed out. It's just who I am.''                                                 \\
		\addlinespace
		\textbf{Counsellor}                                                                                          &
		``It sounds like you really care about setting a good example for your children.''                           &
		``So smoking feels like a part of how you manage difficult emotions.''                                                                                      \\
		\bottomrule
	\end{tabular}
	\caption[Examples of change talk and sustain talk]{Examples of change talk and sustain talk in motivational interviewing, with corresponding counsellor responses. The table shows client statements expressing motivation to change (change talk) and resistance to change (sustain talk), and how a counsellor might respond to each in an MI-consistent manner.}
	\label{tab:change_sustain_talk}
\end{table}



\subsection*{MI Principles and Style}
Underlying MI's conversational approach are several core principles. First, the counsellor should express empathy and use reflective listening to understand the client's perspective and build rapport. Second, MI works to reveal any discrepancy between the client's own goals or values and their current behaviour~\cite{Miller_2023}. Third, the counsellor acknowledges the client's resistance rather than confronting it; resistant remarks are met with understanding and are used as opportunities to examine the client's thoughts further, instead of provoking an argument. Finally, MI supports self-efficacy by emphasizing the client's autonomy and capability in effecting change---the individual is encouraged that they have the strength and choice to quit if they so decide. These principles are often operationalized through specific conversational techniques summarized as OARS: Open-ended questions, Affirmations, Reflective listening, and Summaries~\cite{Rollnick1995}. Examples of how MI counsellors use these skills are given in \Cref{tab:mi_skill_examples}.


MI's empathetic, autonomy-supportive atmosphere is particularly important for \emph{ambivalent smokers}---those who may be defensive or unsure about quitting. It helps reduce resistance and increases engagement in the conversation about change~\cite{Miller1983, MillerRollnick2023}. Notably, MI's strategy of guiding clients to articulate their own arguments for change is grounded in evidence that clients' \emph{change talk} during sessions predicts a greater likelihood of subsequent behaviour change~\cite{MillerRose2009}. Thus, MI sessions explicitly aim to encourage change talk and soften sustain talk, steering the dialogue in a direction where the client's language shifts towards change.



\begin{table}[ht!]
	\centering
	\begin{tabular}{@{} p{0.24\textwidth} p{0.70\textwidth} @{}}
		\toprule
		\textbf{MI Skill}             & \textbf{Example}                                                                                                                                                                                                               \\
		\midrule
		\textbf{Open-ended Question}  &
		``What are some things you've thought about when it comes to cutting back on drinking?''                                                                                                                                                                       \\
		\addlinespace
		\textbf{Affirmation}          &
		``You've shown a lot of strength in coming here today and being open about what's going on.''                                                                                                                                                                  \\
		\addlinespace
		\textbf{Reflective Listening} &
		``So you're feeling stuck. You want to make a change, but you're also worried you might fail again.''                                                                                                                                                          \\
		\addlinespace
		\textbf{Summary}              &
		``Let me see if I've got this right: you've been thinking more about quitting, especially since your health scare, but it's been hard to imagine your daily routine without smoking. At the same time, you've started walking more and cutting back already.'' \\
		\bottomrule
	\end{tabular}
	\caption[Examples of MI skills]{Examples of core skills in motivational interviewing (OARS). The table provides examples for open-ended questions, affirmations, reflective listening, and summaries.}
	\label{tab:mi_skill_examples}
\end{table}




\subsection*{Measuring the Effectiveness of MI Counselling}
As a structured therapeutic approach, MI uses well-defined success criteria. Researchers and clinicians use several strategies to evaluate the quality of MI conversations and their impact on client motivation. One common approach is the Motivational Interviewing Skill Code (MISC), a coding system that categorizes counsellor utterances and client responses to quantify adherence to MI principles~\cite{Houck2010}. Using the MISC, independent annotators (or \emph{coders}) can rate how well a counsellor's statement aligns with MI techniques (for example, by counting reflections, questions, and advice) and determine the proportion of client change talk vs. sustain talk. High MI-consistent scores (e.g., a high ratio of reflections to questions, or a high percentage of client change talk) are associated with better outcomes, and such coding schemes are often used in training and research to ensure the fidelity of MI delivery.

Another practical tool is the ``Readiness Ruler,'' a simple self-reported measure of a client's readiness to change (on a 0-10 scale for readiness, importance, or confidence)~\cite{Boudreaux2012}. In the context of smoking cessation, a counsellor might ask, ``On a scale from 0 to 10, how ready are you to quit smoking?'' The Readiness Ruler is an effective way to track changes in motivation before and after an intervention. For example, an increase in a smoker's readiness score after an MI session would indicate movement toward a decision to quit. Both the MISC coding of session transcripts and readiness scaling of clients are valuable evaluation methods: the former measures the level of motivational language by the client and ensures the conversational style remains true to MI, and the latter provides an outcome-oriented metric of the client's motivational state.

While MI provides the therapeutic approach, the technological means to deliver it at scale in a conversational manner have only recently become available with the advent of large language models.

\subsection{Effectiveness of MI in Smoking Cessation}
MI has been widely adopted in smoking cessation efforts, particularly due to its relevance for smokers who experience ambivalence about quitting. For instance, a national survey found that over half of U.S. smokers express conflicting attitudes toward cessation~\cite{Babb2017}, necessitating interventions that can address such ambivalence.

Over the past two decades, numerous clinical trials and meta-analyses have assessed the efficacy of MI counselling in helping tobacco users quit. A meta-analysis of 31 randomized trials involving over 9,000 smokers reported that MI markedly increased the likelihood of abstinence compared with control conditions, with a pooled odds ratio of approximately 1.45~\cite{Heckman2010}. Similarly, a Cochrane review of 28 studies found that MI-based counselling produced higher six-month quit rates than brief advice, with relative risks ranging from 1.2 to 1.3~\cite{Lindson2015}. While these effect sizes are modest, the evidence consistently suggests that MI improves both quit attempts and abstinence, especially when delivered by trained practitioners in clinical or community settings.

The effectiveness of MI is consistent across various smoking populations and settings. Studies have shown positive outcomes with MI delivered by various professionals (physicians, nurses, trained counsellors) and in formats ranging from a single brief session to multiple sessions~\cite{Lindson2015}. Even a short, 15-20 minute MI-based conversation in a primary care visit can measurably boost a smoker's likelihood of quitting relative to no counselling, especially when the practitioner adheres closely to MI principles~\cite{zanjani2008effectiveness}.


One reason MI is particularly effective for smoking cessation is its alignment with the psychology of ambivalence common among smokers. Many smokers acknowledge the health risks of tobacco while simultaneously relying on it for stress relief or as a habitual comfort, resulting in decisional conflict. MI directly engages this ambivalence by creating a non-confrontational space in which smokers can articulate and examine their mixed feelings, ultimately shifting the balance toward change. Empirical evidence suggests that MI not only improves cessation outcomes but also improves intermediate factors such as motivation, readiness to quit, and self-efficacy~\cite{Boudreaux2012, Hettema2005}. These motivational gains are important, as a readiness to quit is a well-established precursor to cessation success~\cite{West2006}.




\section{Foundational Language Models} \label{sec:foundational_models}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The recent improvements in conversational AI have been advanced by foundational language models~\cite{stanfordCRFM2021}: extremely large neural networks pre-trained on vast corpora of text, which can be adapted to many tasks. These models serve as a foundation that can be specialized for specific applications via \emph{fine-tuning}. Modern foundation models almost universally employ the \textbf{transformer} architecture (discussed in \cref{ssec:transformers}) to capture long-range context and dependencies in dialogue. Consequently, this allows LLM-based chatbots to understand and generate coherent multi-turn conversations and provide consistent responses to the client's statements over long contexts (e.g., multiple sessions).

Foundation models are trained with self-supervised objectives on enormous text datasets, learning a broad range of linguistic patterns, factual knowledge, and even elusive interaction norms. The GPT series of models exemplifies how scaling up model size and data leads to \emph{emergent capabilities}. GPT-3~\cite{brown2020language} (175 billion parameters) demonstrated astonishing \emph{few-shot learning}: it can perform a new language task given only a few examples. This few-shot ability is a direct consequence of training on various data at scale, which allows models to perform \emph{in-context learning}, i.e., adapting to a new task described in the prompt. Such capabilities are invaluable for building a therapeutic chatbot. Instead of laboriously collecting and annotating thousands of counselling dialogues to train a model, it is possible to prompt a pre-trained LLM with instructions and examples of MI, and the model will generalize to produce appropriate counsellor responses~\cite{xie-etal-2024-shot-dialogue}. Later in this thesis, a foundation model is prompted with an expert-informed prompt. This method used the model's generative capabilities while allowing the customization of the output to be MI-consistent. This approach follows a broader trend in NLP: using large pretrained models as a base and conditioning them via prompts or lightweight fine-tuning to perform specialized dialogue tasks~\cite{10.5555/3600270.3602070}.

While OpenAI's GPT models were pioneers, the field of foundational models has rapidly expanded, creating a competitive and varied environment. Other major technology firms have developed their own flagship models, often with distinct architectural philosophies and target applications. Google's Gemini family, for instance, was designed from the ground up to be natively multimodal, processing text, images, audio, and video as interleaved sequences, a departure from models that handle modalities via separate components~\cite{team2023gemini}. Anthropic's Claude series, particularly the Claude 3 models, has pushed the state of the art on benchmark performance while being developed with a strong emphasis on safety through methods like Constitutional AI; these models are also noted for their exceptionally large context windows~\cite{anthropic2024claude}.

A notable counter-trend to these proprietary, closed-weight models has been the proliferation of high-performance open-weight alternatives. Meta has been a leader in this domain, with its Llama series~\cite{touvron2023llama} providing researchers and developers with high-performance base models that can be studied and fine-tuned with transparency. This movement has catalyzed innovation across academia and industry. Similarly, organizations from other countries have made substantial contributions; for example, China's DeepSeek-AI has released a series of proficient open-source models, including specialized variants for code generation and models built on Mixture-of-Experts (MoE) architectures~\cite{deepseek2024deepseekv2}.

The diversification extends beyond just scale. Microsoft has investigated the development of smaller, more curated models with its Phi series and has demonstrated that models trained on high-quality, ``textbook-like'' data can achieve remarkable performance on reasoning tasks, challenging the ``bigger is always better'' assumption~\cite{li2023textbooks}. Concurrently, specialized models targeting niche domains have emerged. For instance, xAI's Grok is designed to use real-time information from the X social media platform~\cite{xai2023grok}, while enterprise-focused offerings like Amazon's Titan~\cite{aws2023titan} and IBM's Granite~\cite{ibm2023granite} prioritize data provenance, reliability, and integration within their respective cloud ecosystems. This rich array of foundational models, each with its  own philosophy on openness, scale, and specialization, provides a broad array of tools that can be adapted for highly specific and sensitive applications.

This is a promising development for therapeutic chatbots: as these models advance, a carefully adapted version can exhibit even more natural dialogue and advanced motivational strategies. At the same time, research into controllability, i.e., giving developers and clinicians the tools to direct an LLM's behaviour, is growing in importance~\cite{fernandez-etal-2025-lamia}. Techniques like system prompts, chain-of-thought prompting for reasoning~\cite{10.5555/3600270.3602070}, and lightweight policy modelling~\cite{du-etal-2024-rewarding} are some emerging trends to further improve an LLM's use as a goal-oriented dialogue system.


\subsection*{The Transformer Neural Architecture}
\label{ssec:transformers}
Transformers are the architectural backbone of virtually all modern large language models, and they play a central role in the chatbot developed in this work. The transformer architecture~\cite{vaswani2017attention} departed from previous neural network designs by using self-attention as its sole mechanism to ``compute representations of its input and output without using sequence-aligned RNNs or convolution''~\cite{vaswani2017attention}. This allowed parallel processing of each input token in the context to compute its deep context-dependent representation. In a transformer, each input token's representation can be computed by attending to every other token in the context (the complete conversation up until now), which enables the model to capture long-range relationships and contexts. This is implemented through multi-head self-attention layers: the model computes attention weights that represent different types of relationships (e.g., semantic similarity, positional relevance) across the sequence. By stacking multiple self-attention layers, transformers can build very deep representations of text. Crucially, they scale efficiently on parallel hardware because each layer's computations can be parallelized (unlike the sequential nature of RNNs). This scalability has allowed training of extremely large models with hundreds of billions of parameters on massive datasets.


Large generative models like GPT-3~\cite{brown2020language}, GPT-4o~\cite{openai2023gpt4}, and others use transformers that predict text autoregressively, i.e., one token at a time, conditioned upon the preceding context. This autoregressive setup is well-suited for dialogue generation, as the model always conditions on the conversation history when producing the next part of its response.


Empirically, the advent of transformer-based LLMs has yielded dramatic improvements in dialogue systems. Models like PaLM~\cite{chowdhery2022palm} (540 billion parameters) and Google DeepMind's Gemini~\cite{comanici2025gemini25pushingfrontier} employ essentially the same transformer building blocks, but at a greater scale and sometimes with enhancements like sparsity or routing. The general finding is that larger transformers not only produce more fluent text but also exhibit emergent behaviours such as reasoning, abstraction, and skilful dialogue skills that smaller models lack~\cite{52065,berti2025emergentabilitieslargelanguage}.


Before the advent of transformer-based LLMs, however, the development of therapeutic chatbots had a long and varied history.


\section{Chatbots for Talk Therapy}
Computer-based \emph{chatbots} have long been examined as a means to deliver talk therapy through natural language. Early attempts date back to the 1960s, when systems like ELIZA simulated a Rogerian psychotherapist by pattern-matching user prompts and responding with scripted phrases~\cite{Weizenbaum1966}. While ELIZA's author intended it as a trivial demonstration, many users unexpectedly found the experience cathartic, mistaking the program for a genuine empathic listener. This serendipitous use of ELIZA foreshadowed both the potential and limitations of early therapeutic chatbots. In the 1970s, Colby and colleagues developed PARRY, a system that modelled paranoid thought patterns to mimic a patient with paranoid schizophrenia~\cite{Colby1971}. PARRY's ability to engage psychiatrists in text dialogue was striking for its time, but like ELIZA, it relied on manually created rules and keywords rather than any true language understanding. These early systems demonstrated that even simple keyword-driven dialogues could evoke an illusion of conversation, yet they lacked memory, contextual understanding, and flexibility. As such, they could not move beyond superficial interactions.


Through the 1980s and 1990s, progress in \emph{talk therapy chatbots} stagnated. Some researchers, however, turned to expert systems and logic-based approaches for clinical use, but these were not true free-form chatbots. Several publications in the mid-1980s examined whether computers could mimic a psychotherapist's reasoning or assist with brief psychotherapy techniques, often by guiding patients through structured question-answer routines~\cite{Hartman1986,Sampson1986,ServanSchreiber1986}. These systems remained largely rule-based: they followed predetermined scripts or decision trees derived from therapeutic principles, without the ability to truly ``understand'' natural language. A few projects showed modest success. For example, an early interactive program for cognitive-behavioural therapy (CBT) was tested for treating depression~\cite{Selmi1990}. However, research on talk therapy chatbots in this era gained little attention. Nonetheless, the idea of computer-aided therapy persisted. By the late 1990s, researchers began developing digital self-help programs that delivered therapy exercises through a desktop computer. One of the first randomized trials of computerized CBT was an interactive software, Beating the Blues, that demonstrated that guided online CBT could markedly reduce anxiety and depression symptoms~\cite{Proudfoot2003}. These efforts, while not ``chatbots'' in the modern sense, established an important proof of concept: computers could deliver legitimate mental health interventions following psychological approaches, even if early systems were highly scripted and simplistic.


The next generation of therapeutic chatbots emerged in the 2010s alongside advances in natural language processing. These systems often adopted a \emph{hybrid approach}, combining scripted decision flows with modest machine learning components (e.g., classifiers to detect user sentiment or intent). Notable examples include conversational agents for mental health like Woebot~\cite{Fitzpatrick2017} and Wysa~\cite{Chang2024}, which deliver principles of CBT or other interventions via a chat interface. Woebot engaged users with brief daily check-ins and mood tracking, intermixing scripted prompts and pre-written empathetic replies. Its dialogues were structured as a branching tree, augmented by simple natural language processing at certain nodes. For example, when Woebot recognised a word indicating loneliness, it replied with a comforting phrase. In a randomized controlled trial with college students, Woebot markedly reduced self-reported depression symptoms over two weeks compared to an information-only control, illustrating the promise of such automated support~\cite{Fitzpatrick2017}. Similarly, Wysa~\cite{Chang2024}, a CBT-based mental health chatbot, has been evaluated in real-world and clinical settings, with some studies suggesting reductions in depression and anxiety with use. These \emph{rule-based} or \emph{hybrid chatbots} can deliver psycho-education and guide users through therapeutic exercises like breathing or re-framing thoughts. Users often report them as convenient and stigma-free support between or instead of human therapy sessions.

One of the earliest rule-based chatbots for MI-based smoking cessation was developed by \citet{Almusharraf_2019}. The developers took a human-centred design approach and collected free-form responses from 121 smokers to train the chatbot's natural language understanding (NLU) components (or intent classifiers). The chatbot was then tested on 100 additional smokers, who showed a marked increase in confidence to quit smoking---a mean increase of 0.8 points on a 0-10 scale ($p=0.00005$) one week after the interaction. This demonstrated the system's potential to help unmotivated smokers move toward quitting.

To this point, all the chatbots we have discussed are rule-based. The lack of free-form language generation in rule-based chatbots means these bots often repeat canned phrases, which patients can find formulaic or insincere. This contributes to low engagement and high attrition: many users try these chatbots only briefly and discontinue when the interaction feels stagnant or ``bot-like.'' Indeed, \citet{LIMPANOPPARAT2024100081} highlighted that many \emph{rule-based} chatbot interventions suffered from users dropping out early, undermining their long-term effectiveness. In contrast, the exceptional conversational capabilities of LLM-based chatbots may be able to provide an engaging, almost human-like experience to users. As such, the section below focuses on the recent developments in LLM-based chatbots.


\section{LLM-Based Chatbots for Talk Therapy}
Recent breakthroughs in LLMs have started a new wave of development in AI chatbots for talk therapy. Unlike rule-based systems, these models can generate free-form and contextually relevant responses. They can also hold a far more natural and flexible dialogue with users. In the therapy domain, this means a chatbot can potentially \textit{respond to anything} a client says, while maintaining a supportive and goal-oriented stance.

LLM-based chatbots have been shown to provide talk therapy using principles from popular therapy techniques such as MI or CBT~\cite{mahmood-etal-2025-fully,kian2024can,Ye2025}. For example, \citet{shen-etal-2020-counseling} fine-tuned a GPT-2 model to produce \emph{reflections} in an MI counselling style. Human evaluators rated the model-generated reflective statements as slightly higher in quality than real practitioner reflections, suggesting that a well-trained language model can capture the essence of empathic, complex paraphrasing. Although the reflections produced by \citet{shen-etal-2020-counseling} were intended as a training aid for human therapists (to provide examples of good reflective listening) rather than for direct use with patients, this work demonstrated the capacity of transformers to generate novel therapeutic responses that adhere to MI principles. Similarly, \citet{brown2023mi} used LLM-generated reflections in an MI chatbot and showed that the system was more empathetic and led to a higher increase in confidence to quit smoking compared to a previous, non-generative version of the chatbot.


Several fully generative therapeutic chatbots have been developed in the last two years. One notable system is TAMI (Technology-Assisted Motivational Interviewing), a chatbot coach for smoking cessation that integrated transformer-based language understanding and generation~\cite{SAIYED2022121}. TAMI used intent classifiers to recognise user inputs and a transformer model to generate MI-consistent replies, including both simple and complex reflections. In a 2022 pilot trial with 34 smokers, users rated TAMI as highly competent in MI skills, though overall satisfaction with the bot was moderate (3 out of 5). These findings indicated that while the bot successfully employed MI techniques, there remained room to improve the user experience, highlighting the need for even more natural dialogue and empathy.

The latest therapeutic chatbots often use advanced foundational language models, either \emph{fine-tuned} on therapy data or guided via careful prompting. For instance, \textbf{MIcha} is a GPT-4o-based chatbot designed to deliver motivational interviewing for behaviour change~\cite{Meyer2025}. In a randomized controlled trial, MIcha's brief conversations markedly increased users' readiness to change unhealthy behaviours compared to a control (an unprompted GPT-4o instance). On a 0--10 \emph{Readiness to Change}~\cite{BienerAbrams1991} scale, interactions with the \emph{MI-adapted} chatbot led to a mean increase of 0.87 (SD=2.02), compared to the control, which led to an increase of 0.73 (SD=2.05)~\cite{Meyer2025}.
The chatbot was prompted to use MI principles, and the study found that using MI-consistent language in generation not only improved outcomes but also helped mitigate harms like inappropriate advice or user distress. Interestingly, the researchers observed that users fell into distinct interaction styles---``cooperative'' vs. ``resistant''---which influenced conversation outcomes. Such insights hint that LLM chatbots might be able to adapt their approach in real-time to different client personalities, a level of personalization impossible with one-size-fits-all scripts.

Another important work is by \citet{doi:10.1056/AIoa2400802}, who developed \textbf{Therabot} and described it as the first generative AI therapy chatbot to undergo a full clinical trial. Therabot uses a fine-tuned LLaMA-2-70B model to deliver CBT for depression, anxiety, and eating disorders, and was evaluated in a four-week \textbf{randomized controlled trial}. Participants who engaged with Therabot showed substantial reductions in symptom severity: about a 51\% drop in depression symptoms, with anxiety and eating disorder symptoms also markedly decreased relative to a waitlist control (30\% and 18\% drop, respectively). Strikingly, users of the AI agent rated their \emph{therapeutic alliance} with the chatbot on par with the alliance they typically feel with humans. This suggests that a well-designed LLM chatbot, by virtue of highly responsive and understanding dialogue, can engender a sense of rapport and trust close to that of real therapy. The chatbot incorporated safety guardrails and human clinician oversight for crisis situations. For example, Therabot was programmed to recognise signs of a user in acute distress and provide gentle crisis intervention messages while simultaneously alerting human support (with an option to connect to a crisis line). These safeguards illustrate the hybrid approach often taken in practice: using LLMs for free-form therapeutic conversation but backing them with controlled protocols for high-risk scenarios. Overall, the success of Therabot's trial is a strong proof-of-concept that LLM-driven chatbots can deliver clinical-level mental health benefits in practice.

LLMs also have a very long context window, which allows them to recall details from earlier sessions and respond with original reflective statements that make the client feel heard. In the case of multi-session interventions, techniques such as session summarization and retrieval-augmented generation (RAG) can be used to tailor the interaction style for the upcoming session. An approach to this has been presented by \citet{corda2024context}, who used LLMs to generate personalized advice for sleep improvement.


\subsubsection{Applications of LLMs in Improving Therapeutic Chatbots}
In addition to \textbf{generating} coherent, context-aware responses for therapeutic chatbots, LLMs can be used in \textbf{modelling the clients} for training and evaluation purposes. We examine this idea in depth in \cref{sec:background_synthetic_validation}. LLMs have also been employed in \textbf{evaluating} chatbot performance: rather than relying solely on labour-intensive human annotation of transcripts, researchers have begun using LLMs such as GPT-4o to automatically assess whether a conversation adheres to MI principles and even to rate the quality of reflections produced. Early studies show reasonably good agreement between GPT-based evaluators and human judgments~\cite{Scholich2025}.


\subsubsection{Limitations and Challenges in Using LLM-Based Therapeutic Chatbots}
Despite the promising outlook of LLM-based chatbots as therapists, some major challenges are yet to be overcome. One such challenge is the LLM's tendency for \textbf{hallucination}, i.e., to generate plausible-sounding but incorrect or ungrounded statements. In general usage, models like ChatGPT have been found to produce some factual error or fabrication in roughly 20\% of their responses~\cite{Li2023}. In a mental health context, such hallucinations could translate to unhelpful or even harmful guidance. For example, making up an unfounded statistic about a treatment or misinterpreting a user's story in a way that breaks rapport. Solutions being examined include grounding the model in verified psychoeducational content and implementing filters for medical advice~\cite{Amugongo2025RAG}.


There is also a complex issue relating to empathy and authenticity. While LLMs can be prompted to respond with empathetic phrases (and often do so convincingly), some claim that the interaction still 'feels different' from a human, especially over time. The empathy is a ``simulation rather than an attuned human reaction''~\cite{SEITZ2024100067}, which can create a gap in how the support is perceived. For example, a study comparing GPT-3-based chatbots to human therapists found that the bots tended to overuse formulaic, reassuring, and affirming statements, yet did not probe deeply into clients' feelings or ask many detailed questions. Therapists, in contrast, elicited more elaboration from clients and used complex reflections and occasional self-disclosures to build connection~\cite{Scholich2025}. This suggests that current general-purpose LLMs, if used ``out-of-the-box,'' may lean towards a superficial counselling style---politely supportive but missing opportunities to examine the client's experience in depth. In crisis situations, these gaps become even more pronounced: the same study noted that unspecialized chatbots handled scenarios of suicidal ideation or severe distress inadequately, often failing to ask about safety or to encourage seeking professional help.


\section{Persona Creation Using LLMs}
LLMs can be equipped with artificial personas to produce engaging and contextually appropriate behaviour in dialogue systems. Before the advent of LLMs, early work on persona-grounded dialogue demonstrated that conditioning responses on a predefined personal profile can address problems of blandness and inconsistency in chit-chat models. For example, \citet{zhang-etal-2018-personalizing} introduced the PersonaChat dataset where dialogue agents were given \emph{persona profiles} (e.g., ``I have a dog. I like camping.'') and showed that conditioning on such profiles yielded more specific and engaging conversations compared to profile-agnostic models. This persona-conditioning approach was found to improve next-utterance prediction and overall dialogue coherence, as the model maintains a consistent identity throughout an interaction. Subsequent research built on this idea by integrating persona information into both retrieval-based and generative chatbots, confirming that persona grounding can improve user engagement and the realism of agent utterances~\cite{roller-etal-2021-recipes, li-etal-2016-persona}. At the same time, these studies revealed challenges: agents often still produced contradictions to their stated \emph{persona profiles} or reverted to generic responses if the persona was not reinforced strongly~\cite{kim-etal-2020-will, song-etal-2020-generate}. Using pre-trained LLMs for persona creation mitigates this problem to a large extent as the \emph{persona profiles} can be attended to on every token generation.

\subsection{Prompt-Based Persona Conditioning}
A common approach to creating an LLM-based persona is to inject \emph{persona profiles} at inference time with prompt engineering. In this method, the model is steered by a carefully designed prompt that delineates the character's identity, backstory, or speaking style. For instance, a system message might instruct: ``You are a 45-year-old smoker who has tried quitting multiple times and is ambivalent about quitting again.'' This kind of backstory conditioning provides an initial context that biases the LLM's generation towards the persona's perspective. Contemporary instruction-following models like GPT-4o accept system prompts that effectively establish such roles, enabling zero-shot persona adoption without additional training~\cite{10.5555/3600270.3602281}. Empirical work has shown that even concise persona descriptions in the prompt can sometimes markedly influence an LLM's lexical choices, tone, and factual claims in ways consistent with that persona~\cite{madotto-etal-2019-personalizing, liu-etal-2024-evaluating-large}.

In more elaborate setups, designers provide a series of in-context demonstrations, e.g., example dialogues or question-answer pairs that exemplify the persona's behaviour (what might be called \textbf{behavioural exemplification}). By seeing a few turns of a persona in action, the LLM can learn to mimic the speaking style and attitudes illustrated by those examples~\cite{joshi-etal-2023-persona}. This few-shot prompting strategy has been used to install personas ranging from cheerful customer service agents to sarcastic comedians, with qualitative improvements in staying ``in character''~\cite{gururangan-etal-2020-dont}. Prompt-based methods have the advantage of not requiring model fine-tuning, but they rely on the model's context window and can degrade as a conversation progresses and new context pushes out the initial persona prompt.

\subsection{Style Transfer and Fine-Tuned Personas}
Another line of work treats persona adoption as a controllable text style problem. Rather than (or in addition to) conditioning the model on a persona description, one can post-process or constrain generation to match a target style associated with the persona. For example, a base LLM might first generate a candidate response based on conversational context, and then a style transfer model rewrites that response in the voice of a 45-year-old male smoker from a particular background.

Prior research on text style transfer provides tools for altering attributes like formality, sentiment, or dialect while preserving meaning~\cite{niu-bansal-2018-polite, sudhakar-etal-2019-transforming}. These techniques have been extended to dialogue personalization, such as \citet{niu-bansal-2018-polite}, which used a politeness classifier and style-specific language model to make a chatbot consistently polite or rude on demand. Also, \citet{zhang-etal-2018-personalizing} trained a transformer-based dialogue model with style embeddings for persona and emotion to generate stylized responses in one pass. Such style-controlled generation can intensify persona-specific linguistic quirks (choice of words, syntax, formality level) and has shown success in producing responses that human evaluators identify as having a distinct personality~\cite{zhang-etal-2018-personalizing}.

An alternative approach to style transfer is fine-tuning the LLM on persona-specific data. By training on dialogues where an agent consistently speaks with a given persona (or on monologues/writings representative of that persona), the model internalizes the patterns of that character. Fine-tuning was the primary method in early persona-chat systems, often combined with latent variable models to encode persona traits~\cite{li-etal-2016-persona}. For example, \citet{roller-etal-2021-recipes} described fine-tuning a 9B-parameter pre-trained model on the PersonaChat dataset and other persona-annotated dialogues, which yielded a chatbot that humans found more consistent in personality and context coherence than a non-fine-tuned baseline. However, fine-tuning large LLMs for each persona is costly and inflexible; hence, prompt-based steering and modular style transfer are increasingly favoured for flexible persona switching.


\subsection{Retrieval-Augmented Persona Memory}
Maintaining a consistent persona over long interactions is a known difficulty. As conversations stray from the initial topic or exceed the model's context length, the agent may ``forget'' its persona or drift in style. Modern systems address this with retrieval-augmented generation (RAG) techniques, wherein relevant persona information is fetched from an external store and fed into the model at each turn~\cite{shuster2022blenderbot3deployedconversational,xu-etal-2022-beyond}. In a persona-aware RAG pipeline, the agent might have a dedicated memory of persona facts or dialogue history that is indexed (perhaps through a vector database). Prior to each response, the system retrieves the most pertinent persona snippets and prepends them to the LLM's input. \textbf{BlenderBot}~\cite{shuster2022blenderbot3deployedconversational} incorporates a long-term memory component that stores both the user's persona and the bot's own persona, and it learns to decide when to retrieve these memory entries to ground its responses~\cite{shuster2022blenderbot3deployedconversational}. This helps the agent avoid contradicting earlier statements about itself or repeating questions the user has answered. Similarly, Multi-Session Chat by \citet{xu-etal-2022-beyond} uses retrieval to carry a persona across multiple dialogue sessions, ensuring that a chatbot remembers a user's personal details and prior conversations even after many interactions.

Retrieval-based persona conditioning has been reported to improve consistency and factual alignment with the persona profile, though it requires reliable triggering mechanisms to decide when persona memory is needed. Recent research prototypes like \textbf{PersonaRAG} explicitly combine user profiling with RAG to create digital avatars that can ``remember'' and evolve with user interactions~\cite{kimara2025personaaileveragingretrievalaugmentedgeneration}.

Creating a persona is one thing; ensuring it is a faithful representation of the intended character is another. This has led to a variety of evaluation methods.

\section{Evaluation of LLM-Based Persona Creation}
To gauge how well a model represents a persona, researchers have employed various evaluation methods, ranging from automatic metrics to human judgment and psychometric tests. One important aspect is \textbf{logical consistency}: the agent should not produce utterances that conflict with the given persona. To measure this, \citet{welleck-etal-2019-dialogueNLI} introduced the Dialogue Natural Language Inference (DNLI) corpus and associated metrics. DNLI consists of dialogue turns paired with persona sentences, labelled for entailment or contradiction (e.g., given persona: ``I have a dog,'' does the reply entail or contradict it?). By testing outputs against persona statements using NLI models, one can quantify contradiction rates. This approach revealed that vanilla persona-based models often ignore or contradict persona facts. This led to solutions including \emph{unlikelihood training} and \emph{controlled text generation} that penalize inconsistencies~\cite{li-etal-2020-dont, kim-etal-2020-will}. Improved models show lower contradiction rates on DNLI and PersonaChat, indicating better persona fidelity~\cite{kim-etal-2020-will}.

Beyond logical consistency, evaluators examine whether the content and style of the agent's responses align with the expected persona profile. \textbf{Linguistic style matching} is one tool: for example, does a model supposed to embody a high-extraversion persona use more social and positive-emotion words, as real extraverts do? Studies have used resources like the Linguistic Inquiry and Word Count (LIWC) lexicon to analyze generated language for personality markers~\cite{jiang-etal-2023-personallm}. \citet{jiang-etal-2023-personallm} conducted an extensive experiment assigning GPT-4o various Big Five personality profiles and found that the model's word choices and tone shifted in accordance with the target traits (e.g., ``extroverted'' personas produced more talkative, upbeat narratives than ``introverted'' ones). They also had the persona-infused model complete a standard 44-item Big Five Inventory questionnaire in prompt form, and the scores derived from the LLM's answers correlated strongly with the intended trait levels~\cite{jiang-etal-2023-personallm}. This suggests that with the appropriate prompting, an LLM can consistently express designated personality traits to a degree measurable by psychological scales.

%Reliability on logical consistency and linguistic style is not enough: 
The work in \citet{shu-etal-2024-personality-tests} cautions that slight variations in how questions are phrased or ordered can lead to inconsistencies in LLM persona questionnaire results, indicating that current prompting strategies may not always capture a model's ``true'' persona in a reliable way~\cite{shu-etal-2024-personality-tests}. As a complementary approach, \textbf{human evaluation} remains crucial. Researchers often ask human annotators to judge if a conversation excerpt ``sounds like'' it was spoken by the intended persona (e.g., does this really feel like a 45-year-old smoker speaking?). In the PersonaLLM study, humans could correctly identify certain personality traits from a model's stories at rates far above chance (more than 80\% accuracy for clearly manifested traits)~\cite{jiang-etal-2023-personallm}. Similarly, the recent PersonaGym framework~\cite{samuel2025personagymevaluatingpersonaagents} proposes a battery of scenarios where a persona-equipped agent is queried on various tasks (from factual questions to moral dilemmas) and rated (by LLM-based or human evaluators) on whether its responses align with the persona's expected knowledge, behaviour, and preferences. Such multi-dimensional evaluations aim to go beyond surface traits, testing whether the persona remains consistently applied across different contexts and over extended dialogues.


\subsection*{Known Issues with LLM-Based Persona Creation}
Current LLM persona construction techniques face major limitations. One concern is the reinforcement of stereotypes and ``flattened'' personas. The work in \citet{liu-etal-2024-evaluating-large} highlights that when an LLM is asked to embody a persona with incongruous or uncommon trait combinations (for example, a persona that is a political liberal but supports a traditionally conservative policy), the model often defaults to the stereotype---producing opinions more congruent with typical liberals or typical supporters of that policy, rather than faithfully holding the unusual combination of views. They found a nearly 10\% drop in steerability for such incongruous personas, with the model sometimes slipping into the demographically expected stance instead of the target stance. This indicates that LLMs have difficulty representing complex, less common identities, tending instead to regress to more stereotypical patterns present in their training data.

Another limitation is maintaining coherence over long interactions. The context window of even modern LLMs (e.g., 4K to 32K tokens) is finite, which means lengthy conversation sessions may push the initial persona prompt out of scope. Without special handling, the model may start deviating from its role after many turns. Memory-augmented strategies as described above only partially mitigate this; errors can accumulate if the retrieval mechanism brings back irrelevant or incomplete persona details. Ensuring that a persona's voice and knowledge remain steady over hours of conversation is an open challenge.

There is also the issue that LLM-simulated personas lack a genuine internal life or the ability to experience emotions, which can lead to shallow or inconsistent modelling of complex human traits like empathy, remorse, or motivation. A persona might verbally claim to be ``depressed'' or ``highly motivated,'' but the model does not feel these states, potentially yielding dialogue that rings hollow or fails to adapt when tested in emotionally charged situations. \citet{gupta-etal-2024-sociodemographic} show that persona assignment can introduce hidden biases in reasoning. For instance, an LLM role-playing as an aggressive character might systematically favour combative responses in moral reasoning tasks, raising concerns that persona-conditioned LLMs could magnify certain biases under the guise of ``staying in character.'' Ethically, developers must be careful that personas do not become a vehicle for harmful or unfair stereotypes (e.g., a ``mentally ill persona” that unintentionally produces stigmatizing language). Transparent documentation of how a persona is constructed and what its limits are is recommended when deploying persona-driven bots in sensitive domains~\cite{wu2025personastalksrevisitingimpact}.

The ability to create and evaluate personas is a stepping stone towards a more ambitious goal: creating synthetic subjects that can act as stand-ins for humans in behavioural experiments.




\section{LLM-Based Synthetic Subjects (Doppelgängers) in Behavioural Experiments}
\label{sec:background_synthetic_validation}

The extent to which LLMs can emulate human behaviour has become a central question in computational social science and AI research. One promising approach to assess how closely LLMs behave compared to humans is to replicate human behavioural experiments using LLM-based synthetic agents, each configured with a persona derived from a human participant in a study. We refer to such a synthetic agent as the participant's \emph{doppelgänger}. In principle, a doppelgänger can substitute for the human subject, enabling faster, safer, and more scalable experimentation. In practice, however, creating effective doppelgängers presents several challenges: (1) persona profiles may lack sufficient richness to capture the full complexity of human behaviour; (2) LLMs may fail to consistently adhere to the installed persona; and (3) the model's internal knowledge may dominate or distort the intended profile, leading to exaggeration or stereotype perpetuation. As a result, both the construction and rigorous validation of doppelgängers, i.e., scientifically measuring how closely they reproduce the responses of their human counterparts, are important. In the following sections, we examine a range of recent studies that have attempted to create LLM-based doppelgängers in disparate experimental domains (e.g., social, behavioural, clinical) and critique their approach to the creation and validation of such doppelgängers.


\subsection*{LLM Doppelgängers in Social Surveys}
\citet{argyle2023} introduced the concept of ``silicon samples'' as proxies for human survey respondents. They conditioned GPT-3 with thousands of real participants' demographic backstories (e.g. age, gender, race, education, political affiliation) from U.S. survey data. They then generated GPT-3's answers to the same questionnaires people had answered. The validation of these simulated respondents was quantitative: they measured how closely the distribution of answers from the LLM doppelgängers matched the actual human survey distributions across many items and correlations. Notably, the GPT-3 doppelgängers exhibited high \textit{algorithmic fidelity}---meaning they reproduced not only overall response proportions but also subgroup differences and attitude inter-correlations present in the human data. For example, ``a conservative older male'' persona in the model would respond to political questions in line with real conservatives of that demographic, while a ``liberal young female'' persona's simulated responses aligned with that group's patterns.

\citet{argyle2023}'s validation method was statistical and aggregate. The authors had to correct some skewed marginals in the model's raw outputs to better align with known sample proportions, indicating that the LLM did not automatically produce a perfectly representative sample without adjustment. More importantly, demonstrating that an LLM can match population-level distributions does not guarantee that any single doppelgänger behaves indistinguishably from its human counterpart in an interactive setting. The questions were mostly closed-ended survey items rather than free-flowing conversations, so it remains unclear whether the same fidelity would hold in open-ended behavioural or linguistic responses. In short, while \citet{argyle2023} provided evidence that LLMs can emulate group-level attitudes with impressive granularity, their validation method (comparing survey response patterns) overlooks dynamic interaction traits and relies on the assumption that distributional similarity implies a faithful reproduction of individual human behaviour. This leaves a gap when considering applications like therapy dialogues where moment-by-moment language use matters.



\subsection{Current Validation Methods for LLM Doppelgängers}


Although prior works have made important strides in developing LLM-based doppelgängers and inventing ways to validate them, their methods often emphasize either macro-level correspondence (distributions and outcomes) or subjective appraisal. None of the existing studies in the behavioural or health context rigorously validated the \emph{process} of the synthetic subject's behaviour using domain-specific behavioural metrics. This is where our thesis work seeks to contribute a novel approach. We install richly detailed smoking-related personas into an LLM (drawing on real smokers' demographics, dependence levels, and motivation scores) and then validate these doppelgängers by analysing their conversational output using Motivational Interviewing (MI) metrics that have known relevance to behaviour change. Specifically, we evaluate the language of the synthetic smokers for its balance of \emph{change talk} vs. \emph{sustain talk} and we examine their pre- and post-conversation \emph{readiness-to-change} ratings. These measures provide an objective, theory-driven way to assess whether the doppelgängers are behaving like real smokers at a given stage of change. For example, a genuine smoker ambivalent about quitting will often produce a low fraction of change talk (more sustain talk defending the status quo) and a low readiness score; our validation checks if the corresponding LLM doppelgänger exhibits a similar profile. By correlating the doppelgängers' MI-consistent behaviours with those of their human counterparts and comparing distributions of metrics like $C/(C+S)$ talk ratios (change talk fraction) across groups, we can quantitatively gauge fidelity in the very domain of interest---something prior work did not tackle. 

Unlike \citet{argyle2023}'s demographic backstories or \citet{aher2023}'s game outcomes, our validation focuses on conversational indicators of motivation, which are specialized and domain-specific. Similarly, in contrast to the general plausibility ratings used by \citet{Cook2025} and \citet{Haider2025}, our approach uses validated psycholinguistic coding (e.g., the MISC scheme for MI dialogue) to measure the doppelgänger's behaviour against established human benchmarks for ``change talk'' frequency. We also borrow the concept of a readiness ruler from clinical psychology and ask the synthetic smokers to self-report their readiness to quit before and after a counselling dialogue --mirroring exactly what the real participants did --- to see if the LLM's self-assessment shifts in tandem with the human's, and if not, where the discrepancies lie.


So far, we have established the foundations for this work, drawing from two distinct but converging areas: the clinical science of Motivational Interviewing and the engineering of Large Language Models. We reviewed MI as an evidence-based method for smoking cessation that guides individuals to articulate their own reasons for change. Its effectiveness is measurable through specific behavioural indicators like the balance of client ``change talk'' versus ``sustain talk'' and shifts in self-reported readiness scores. We also traced the evolution of therapeutic chatbots, from early rule-based systems to modern generative agents powered by LLMs, which offer unprecedented conversational fluidity.

The creation of synthetic agents introduces a significant challenge: how can we confidently test and validate these systems? The recent practice of using LLMs to create synthetic ``doppelgängers'' of human participants for behavioural research offers a path forward, but existing validation techniques often lack rigour. Methods that rely on aggregate statistical matching or subjective plausibility ratings do not confirm whether a synthetic subject behaves realistically within the specific dynamics of a therapeutic conversation.

This thesis confronts this validation challenge directly. We propose and implement a novel methodology centred on creating MIBot, an MI-based chatbot for smoking cessation, and testing it against a cohort of synthetic smoker doppelgängers. Our primary contribution is a new validation framework. Instead of using generic benchmarks, we validate our synthetic smokers using the core, theory-driven metrics of MI itself.  This domain-specific validation creates a reliable experimental environment for developing automated therapeutic agents. Having laid this conceptual groundwork, the next chapter moves from theory to practice, detailing the design, development, and deployment of MIBot and the construction of the synthetic smoker personas used in our study.